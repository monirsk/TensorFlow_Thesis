

Module 1:

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


# Defining KNN

import numpy as np

class KNN:
    def __init__(self, k=3):
        self.k = k

    def fit(self, X, y):
        self.X_train = X
        self.y_train = y

    def compute_distances(self, x):
        differences = (self.X_train - x)**2
        sum_squared = np.sum(differences, axis=1)     #distance from a single test point to each training sample, basically row wise jog korbe
        distances = np.sqrt(sum_squared)
        return distances


    def get_k_nearest_neighbors(self, distances):
        sorted_indices = np.argsort(distances)
        k_indices = sorted_indices[:self.k]
        return k_indices

    def majority_vote(self, k_indices):
        k_nearest_labels = []
        for i in k_indices:
            label = self.y_train[i]
            k_nearest_labels.append(label)

        label_counts = {}
        for label in k_nearest_labels:
            if label in label_counts:
                label_counts[label] += 1
            else:
                label_counts[label] = 1


        majority_label = None
        max_count = -1
        for label in label_counts:
            if label_counts[label] > max_count:
                max_count = label_counts[label]
                majority_label = label
        return majority_label



    def _predict_single_point(self, x):
        distances = self.compute_distances(x)
        k_indices = self.get_k_nearest_neighbors(distances)
        prediction = self.majority_vote(k_indices)

        return prediction

    def predict(self, X):
        predictions = []
        for x in X:
            label = self._predict_single_point(x)
            predictions.append(label)

        return np.array(predictions)



url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
column_names = [
    "Pregnancies", "Glucose", "BloodPressure", "SkinThickness",
    "Insulin", "BMI", "DiabetesPedigreeFunction", "Age", "Outcome"
]
df = pd.read_csv(url, header=None, names=column_names)



X = df.drop("Outcome", axis=1).values
y = df["Outcome"].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

knn = KNN(k=13)
knn.fit(X_train, y_train)

# Prediction on test data
y_pred = knn.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")




*************************************Module 1: Second Part


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


# Defining KNN

import numpy as np

class KNN:
    def __init__(self, k=3):
        self.k = k

    def fit(self, X, y):
        self.X_train = X
        self.y_train = y

    def compute_distances(self, x):
        differences = (self.X_train - x)**2
        sum_squared = np.sum(differences, axis=1)     #distance from a single test point to each training sample, basically row wise jog korbe
        distances = np.sqrt(sum_squared)
        return distances


    def get_k_nearest_neighbors(self, distances):
        sorted_indices = np.argsort(distances)
        k_indices = sorted_indices[:self.k]
        return k_indices

    def majority_vote(self, k_indices):
        k_nearest_labels = []
        for i in k_indices:
            label = self.y_train[i]
            k_nearest_labels.append(label)

        label_counts = {}
        for label in k_nearest_labels:
            if label in label_counts:
                label_counts[label] += 1
            else:
                label_counts[label] = 1


        majority_label = None
        max_count = -1
        for label in label_counts:
            if label_counts[label] > max_count:
                max_count = label_counts[label]
                majority_label = label
        return majority_label



    def _predict_single_point(self, x):
        distances = self.compute_distances(x)
        k_indices = self.get_k_nearest_neighbors(distances)
        prediction = self.majority_vote(k_indices)

        return prediction

    def predict(self, X):
        predictions = []
        for x in X:
            label = self._predict_single_point(x)
            predictions.append(label)

        return np.array(predictions)



url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
column_names = [
    "Pregnancies", "Glucose", "BloodPressure", "SkinThickness",
    "Insulin", "BMI", "DiabetesPedigreeFunction", "Age", "Outcome"
]
df = pd.read_csv(url, header=None, names=column_names)



X = df.drop("Outcome", axis=1).values
y = df["Outcome"].values


import random
noise_ratio = 0.1  # 10% noise
y_noisy = y.copy()
n_samples = int(len(y) * noise_ratio)

indices = random.sample(range(len(y)), n_samples)
for idx in indices:
    # Flipping the binary labels
    y_noisy[idx] = 1 - y_noisy[idx]

# Split the noisy dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y_noisy, test_size=0.2, random_state=42)

# Initialize the KNN classifier
knn = KNN(k=13)

# Train the classifier on the noisy training data
knn.fit(X_train, y_train)

# Predict on the noisy test data
y_pred = knn.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy for distorted dataset: {accuracy * 100:.2f}%")





######################################################################################## Module 2 #########################################################################
***********Perceptron V_1
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score


# Initialization of weights and parameters
def initialize_weights(n_inputs):
    weights = np.random.uniform(-0.01, 0.01, n_inputs + 1)
    return weights

def step_function(z):
    return 1 if z > 0 else 0

# Training the perceptron
def train_perceptron_1(X, y, weights, threshold= 0.5, n_iterations=100):
    weights[0] = -threshold
    num_samples = X.shape[0]
    bias_column = np.ones((num_samples, 1))
    X = np.concatenate((bias_column, X), axis=1)

    for iteration in range(n_iterations):
        for i in range(len(X)):
            xi = X[i]
            target = y[i]
            weighted_sum = np.dot(weights, xi)
            y_pred = step_function(weighted_sum)


            if target == 1 and y_pred == 0:
                weights +=  xi
            elif target == 0 and y_pred == 1:
                weights -=  xi


    return weights

def predict_perceptron_1(X_test, weights):
    num_samples = X_test.shape[0]             
    bias_column = np.ones((num_samples, 1)) 
    X_test_with_bias = np.concatenate((bias_column, X_test), axis=1)

    predictions = []

    for i in range(num_samples):
        xi = X_test_with_bias[i]
        dot_product = np.dot(weights, xi)
        output = step_function(dot_product)
        predictions.append(output)

    return predictions

data = pd.read_csv('AND_DATASET_Custom.csv', header=0 )

X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the perceptron
n_inputs = X.shape[1]
weights = initialize_weights(n_inputs)
weights = train_perceptron_1(X_train, y_train, weights, n_iterations=100)


# # Test the perceptron and evaluate accuracy
y_pred = predict_perceptron_1(X_test, weights)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy on test set (AND_DATASET_Custom):", accuracy*100, "%")


**************************************************************** Perceptron V_2 (learning rate)

**************************************************************** Widrow_Holf Perceptron

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score




def initialize_weights(n_inputs):
    weights = np.random.uniform(-0.01, 0.01, n_inputs + 1)
    return weights

def step_function(z):
    return 1 if z > 0 else 0

def train_widrow_hoff(X, y, weights, threshold,learning_rate, n_iterations=100):

    num_samples = X.shape[0]
    bias_column = np.ones((num_samples, 1))
    X = np.concatenate((bias_column, X), axis=1)
    weights[0] = -threshold


    for iteration in range(n_iterations):
        for i in range(num_samples):
            xi = X[i]
            d = y[i]

    
            weighted_sum = np.dot(weights, xi)
            y_pred = step_function(weighted_sum)

    
            error = d - y_pred

            # using the Widrow-Hoff rule
            weights = weights + learning_rate * error * xi

    return weights

def predict_perceptron(X_test, weights):
    num_samples = X_test.shape[0]             
    bias_column = np.ones((num_samples, 1)) 
    X_test_with_bias = np.concatenate((bias_column, X_test), axis=1)

    predictions = []

    for i in range(num_samples):
        xi = X_test_with_bias[i]
        dot_product = np.dot(weights, xi)
        output = step_function(dot_product)
        predictions.append(output)

    return predictions


data = pd.read_csv('AND_DATASET_Custom.csv', header=0 )
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the perceptron
n_inputs = X.shape[1]
weights = initialize_weights(n_inputs)
weights = train_widrow_hoff(X_train, y_train, weights, threshold = 0.5, learning_rate=.01, n_iterations=100)


# testing
y_pred = predict_perceptron(X_test, weights)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy on test set (AND_DATASET_Custom):", accuracy*100, '%')



########################################################################################################### Module 3
import numpy as np

def initialize_weights(input_size, hidden_size, output_size):
    W1 = np.random.randn(hidden_size, input_size + 1) * 0.01
    W2 = np.random.randn(output_size, hidden_size + 1) * 0.01
    return W1, W2

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

def forward(X, W1, W2):
    X = np.insert(X, 0, 1)  # Bias input
    hidden_input = np.dot(W1, X)
    hidden_output = sigmoid(hidden_input)
    
    hidden_output = np.insert(hidden_output, 0, 1)  # Bias in hidden layer
    final_input = np.dot(W2, hidden_output)
    final_output = sigmoid(final_input)
    return X, hidden_input, hidden_output, final_input, final_output

def backward(X, y, W1, W2, hidden_output, final_output, learning_rate):
    output_error = (y - final_output) * sigmoid_derivative(final_output)
    hidden_error = sigmoid_derivative(hidden_output[1:]) * np.dot(W2[:, 1:].T, output_error)

    W2 += learning_rate * np.outer(output_error, hidden_output)  # Update output weights
    W1 += learning_rate * np.outer(hidden_error, X)  # Update hidden weights

    return W1, W2

def compute_accuracy(X_test, y_test, W1, W2):
    correct = 0
    for X, y in zip(X_test, y_test):
        _, _, _, _, output = forward(X, W1, W2)
        predicted = 1 if output >= 0.5 else 0
        if predicted == y:
            correct += 1
    return (correct / len(X_test)) * 100

def train(X_train, y_train, W1, W2, learning_rate, epochs=3000):
    for epoch in range(epochs):
        for X, y in zip(X_train, y_train):
            X, hidden_input, hidden_output, final_input, final_output = forward(X, W1, W2)
            W1, W2 = backward(X, y, W1, W2, hidden_output, final_output, learning_rate)

        if epoch % 1000 == 0:
            accuracy = compute_accuracy(X_train, y_train, W1, W2)
            print(f"Epoch {epoch}: Accuracy = {accuracy:.2f}%")

    return W1, W2


import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

if __name__ == "__main__":
    xor_dataset = pd.read_csv('xor_dataset.csv')

    X = xor_dataset[['Input1', 'Input2']].values
    y = xor_dataset['Output'].values
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    input_size = 2
    hidden_size = 8
    output_size = 1
    learning_rate = 0.5

    W1, W2 = initialize_weights(input_size, hidden_size, output_size)
    W1, W2 = train(X_train, y_train, W1, W2, learning_rate)

    # Compute accuracy on test set
    final_accuracy = compute_accuracy(X_test, y_test, W1, W2)
    print(f"Accuracy on Test Set: {final_accuracy:.2f}%\n")

    for X in X_test:
        _, _, _, _, output = forward(X, W1, W2)
        predicted = 1 if output >= 0.5 else 0
        print(f"Input: {X}, Predicted: {predicted}")




################################################################################ Module 4(a)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

def initialize_weights(input_dim, map_dim):    #  input_dim = 4 features (sepal length,width, petal length, width)
    return np.random.rand(map_dim[0], map_dim[1], input_dim) * 0.1

def calculate_distances(weights, input_vector):

    num_rows, num_cols, num_features = weights.shape
    distances = np.zeros((num_rows, num_cols))
    for i in range(num_rows):
        for j in range(num_cols):
            distances[i, j] = np.sqrt(np.sum((weights[i, j] - input_vector) ** 2))

    return distances

    

def find_bmu(distances):

    min_index = np.argmin(distances)
    bmu_row = min_index // distances.shape[1]
    bmu_col = min_index % distances.shape[1]

    return (bmu_row, bmu_col)



def neighborhood_radius(initial_radius, iteration, num_iterations):
    return initial_radius * (1 - iteration / num_iterations)

def update_weights(weights, input_vector, bmu_index, iteration, learning_rate):
    #Update weights for the BMU and its neighbors
    bmu_x, bmu_y = bmu_index
    radius = neighborhood_radius(initial_radius, iteration, num_iterations)

    for x in range(weights.shape[0]):
        for y in range(weights.shape[1]):
            distance_to_bmu = np.linalg.norm(np.array([bmu_x, bmu_y]) - np.array([x, y]))

            if distance_to_bmu <= radius:
                learning_rate_effective = learning_rate * (1 - iteration / num_iterations)
                weights[x, y] += learning_rate_effective * (input_vector - weights[x, y])

def train_som(input_data, input_dim, map_dim, learning_rate=0.1, initial_radius=1.0, num_iterations=100):
    weights = initialize_weights(input_dim, map_dim)
    for iteration in range(num_iterations):
        for input_vector in input_data:
            distances = calculate_distances(weights, input_vector)
            bmu_index = find_bmu(distances)
            update_weights(weights, input_vector, bmu_index, iteration, learning_rate)

    return weights


# Load the Iris dataset
iris = load_iris()
input_data = iris.data
target = iris.target
input_dim = input_data.shape[1]
map_dim = (2,2)
learning_rate = 0.1
initial_radius = 5.0
num_iterations = 100


scaler = StandardScaler()
input_data = scaler.fit_transform(input_data)

trained_weights = train_som(input_data, input_dim, map_dim, learning_rate, initial_radius, num_iterations)

# Dimensionality reduction for visualization
pca = PCA(n_components=2)
input_data_2d = pca.fit_transform(input_data)

# Visualization of the clusters and SOM nodes
def plot_clusters_and_som(input_data_2d, target, weights):
    plt.figure(figsize=(12, 8))

    scatter = plt.scatter(input_data_2d[:, 0], input_data_2d[:, 1], c=target, cmap='viridis', alpha=0.6, edgecolor='k', s=100)

    for x in range(weights.shape[0]):
        for y in range(weights.shape[1]):
            weight_vector = weights[x, y]
            weight_vector_2d = pca.transform(weight_vector.reshape(1, -1))
            plt.scatter(weight_vector_2d[0, 0], weight_vector_2d[0, 1], c='red', marker='X', s=200, edgecolor='k')

    plt.title('Kohonen Self-Organizing Map (SOM) - Iris Dataset Clusters')
    plt.xlabel('PCA Component 1')
    plt.ylabel('PCA Component 2')
    plt.clim(-0.5, 2.5)
    plt.grid()
    plt.show()

plot_clusters_and_som(input_data_2d, target, trained_weights)





################################################################################ Module 4(B)




import numpy as np

def step_function(x):
    return 1 if x >= 0 else -1

class HopfieldNetwork:
    def __init__(self, num_nodes):
        self.num_nodes = num_nodes
        self.weights = np.zeros((num_nodes, num_nodes))

    def train(self, patterns):
        """Assign connection weights using Hebbian learning."""
        for p in patterns:
            p = np.array(p)
            self.weights += np.outer(p, p)
        np.fill_diagonal(self.weights, 0)  # Set diagonal to 0 (no self-connections)

    def run(self, input_pattern, max_iterations=100):
        """Run the network until convergence."""
        state = np.array(input_pattern)
        for _ in range(max_iterations):
            new_state = np.copy(state)
            for i in range(self.num_nodes):
                net_input = np.dot(self.weights[i], state)
                new_state[i] = step_function(net_input)
            if np.array_equal(state, new_state):
                break  # Converged
            state = new_state
        return state

# Example usage:
if __name__ == "__main__":
    # Define training patterns (binary vectors of +1 and -1)
    patterns = [
        [+1, -1, +1, -1],
        [-1, +1, -1, +1],
        [+1, +1, +1, +1],
        [-1, -1, -1, -1],
        [+1, +1, -1, -1],
        [+1, +1, +1, -1],
        [-1, -1, +1, +1],
        [-1, -1, -1, +1]

    ]

    # Initialize Hopfield network with the size of patterns
    hopfield_net = HopfieldNetwork(num_nodes=4)
    hopfield_net.train(patterns)

    # Input a noisy pattern (unknown pattern)
    test_input = [-1, -1, -1, -1]
    output = hopfield_net.run(test_input)

    print("Input pattern: ", test_input)
    print("Output pattern:", output.tolist())
